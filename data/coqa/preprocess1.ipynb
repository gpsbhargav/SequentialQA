{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "context_history_size = 2\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-train-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_train.pkl\"\n",
    "    \n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-dev-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_dev.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return [x.text for x in nlp.tokenizer(normalize(text)) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text):\n",
    "    paragraph_out = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        paragraph_out.append(word_tokenize(s))\n",
    "    return paragraph_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_overlap(sent,ans):\n",
    "    sent_tok = set(sent)\n",
    "    ans_tok = set(ans)\n",
    "    return 100 - len(ans_tok.difference(sent_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sentence(passage, span):\n",
    "    matching_sentence = [0 for i in range(len(passage))]\n",
    "    matching_scores = []\n",
    "    for sent in passage:\n",
    "        matching_scores.append(score_overlap(sent, span))\n",
    "    best_match_index = np.array(matching_scores).argmax()\n",
    "    matching_sentence[best_match_index] = 1\n",
    "    return matching_sentence\n",
    "\n",
    "def get_prev_ids(current_index, context_history_size, turn_id):\n",
    "    ids = list(range(current_index-context_history_size, current_index))\n",
    "    assert(turn_id != 0)\n",
    "    for i in range(context_history_size-turn_id+1):\n",
    "        ids[i]=0\n",
    "    return ids\n",
    "\n",
    "def make_tables(dataset_in, context_history_size):\n",
    "    passages = []\n",
    "    questions = [[]]  # add a blank question. Use this as history when nothing is availabe\n",
    "    answer_spans = [[]] # add a blank answer. Use this as history when nothing is availabe\n",
    "    answer_sentences = []\n",
    "    data_points = []\n",
    "    for passage in tqdm(dataset_in['data']):\n",
    "        passage_sents = sent_tokenize(normalize(passage['story']))\n",
    "        passages.append(passage_sents)\n",
    "        for i in range(len(passage['questions'])):\n",
    "# data_point format: (passage_id, question_id, [prev qa_ids])\n",
    "            d_p = []\n",
    "            question = word_tokenize(normalize(passage['questions'][i]['input_text']))\n",
    "            question_turn = passage['questions'][i]['turn_id']\n",
    "            ans_span = word_tokenize(normalize(passage['answers'][i]['span_text']))\n",
    "            ans_sent = find_matching_sentence(passages[-1], ans_span)\n",
    "            \n",
    "            questions.append(question)\n",
    "            answer_spans.append(ans_span)\n",
    "            answer_sentences.append(ans_sent)\n",
    "            \n",
    "            d_p.append(len(passages)-1)\n",
    "            d_p.append(len(questions)-1)\n",
    "            \n",
    "            prev_qa_ids = get_prev_ids(current_index=len(questions)-1, \n",
    "                                       context_history_size=context_history_size,\n",
    "                                       turn_id=question_turn)\n",
    "            d_p.append(prev_qa_ids)\n",
    "            data_points.append(d_p) \n",
    "    return {\"passages\":passages, \"questions\":questions, \"answer_spans\":answer_spans, \n",
    "            \"answer_sentences\":answer_sentences, \"data_points\":data_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset_original = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:13<00:00, 42.78it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_formatted = make_tables(dataset_original, context_history_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation: number of questions and answer_spans is one more than number of answer_sentences and data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "7984\n",
      "7984\n",
      "7983\n",
      "7983\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_formatted['passages']))\n",
    "print(len(dataset_formatted['questions']))\n",
    "print(len(dataset_formatted['answer_spans']))\n",
    "print(len(dataset_formatted['answer_sentences']))\n",
    "print(len(dataset_formatted['data_points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['passages'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['questions'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['answer_spans'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['answer_sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['data_points'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path,out_pkl_name,dataset_formatted)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
