{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "context_history_size = 2\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-train-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_train.pkl\"\n",
    "    \n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-dev-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_dev.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return [x.text for x in nlp.tokenizer(normalize(text)) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text):\n",
    "    paragraph_out = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        paragraph_out.append(word_tokenize(s))\n",
    "    return paragraph_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_overlap(sent,ans):\n",
    "    sent_tok = set(sent)\n",
    "    ans_tok = set(ans)\n",
    "    return 100 - len(ans_tok.difference(sent_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sentence(passage, span):\n",
    "    matching_sentence = [0 for i in range(len(passage))]\n",
    "    matching_scores = []\n",
    "    for sent in passage:\n",
    "        matching_scores.append(score_overlap(sent, span))\n",
    "    best_match_index = np.array(matching_scores).argmax()\n",
    "    matching_sentence[best_match_index] = 1\n",
    "    return matching_sentence\n",
    "\n",
    "def get_prev_ids(current_index, context_history_size, turn_id):\n",
    "    ids = list(range(current_index-context_history_size, current_index))\n",
    "    assert(turn_id != 0)\n",
    "    for i in range(context_history_size-turn_id+1):\n",
    "        ids[i]=0\n",
    "    return ids\n",
    "\n",
    "def make_tables(dataset_in, context_history_size):\n",
    "    separator = ['<separator>']\n",
    "    passages = []\n",
    "    questions = [[]]  # add a blank question. Use this as history when nothing is availabe\n",
    "    answer_spans = [[]] # add a blank answer. Use this as history when nothing is availabe\n",
    "    answer_sentences = []\n",
    "    data_points = []\n",
    "    turn_ids = []\n",
    "    histories = [[]]\n",
    "    for passage in tqdm(dataset_in['data']):\n",
    "        passage_sents = sent_tokenize(normalize(passage['story']))\n",
    "        passages.append(passage_sents)\n",
    "        for i in range(len(passage['questions'])):\n",
    "# data_point format: (passage_id, question_id, [prev qa_ids])\n",
    "            d_p = []\n",
    "            question = word_tokenize(normalize(passage['questions'][i]['input_text']))\n",
    "            question_turn = passage['questions'][i]['turn_id']\n",
    "            rationale = word_tokenize(normalize(passage['answers'][i]['span_text']))\n",
    "            ans_sent = find_matching_sentence(passages[-1], rationale)\n",
    "            ans_span = passages[-1][ans_sent.index(1)]  # __.index(1) gives the location of 1 in the list.\n",
    "            \n",
    "            turn  = passage['questions'][i]['turn_id']\n",
    "            turn_ids.append(turn)\n",
    "            \n",
    "            questions.append(question)\n",
    "            answer_spans.append(ans_span)\n",
    "            histories.append(question + separator + ans_span)\n",
    "            answer_sentences.append(ans_sent)\n",
    "            \n",
    "            d_p.append(len(passages)-1)\n",
    "            d_p.append(len(questions)-1)\n",
    "            \n",
    "            prev_qa_ids = get_prev_ids(current_index=len(questions)-1, \n",
    "                                       context_history_size=context_history_size,\n",
    "                                       turn_id=question_turn)\n",
    "            d_p.append(prev_qa_ids)\n",
    "            data_points.append(d_p) \n",
    "    return {\"passages\":passages, \"questions\":questions, \"answer_spans\":answer_spans, \n",
    "            \"answer_sentences\":answer_sentences, \"data_points\":data_points, \"turn_ids\":turn_ids, \n",
    "            'histories':histories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset_original = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:13<00:00, 38.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_formatted = make_tables(dataset_original, context_history_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation: number of questions and answer_spans is one more than number of answer_sentences and data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "7984\n",
      "7984\n",
      "7984\n",
      "7983\n",
      "7983\n",
      "7983\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_formatted['passages']))\n",
    "print(len(dataset_formatted['questions']))\n",
    "print(len(dataset_formatted['answer_spans']))\n",
    "print(len(dataset_formatted['histories']))\n",
    "print(len(dataset_formatted['answer_sentences']))\n",
    "print(len(dataset_formatted['data_points']))\n",
    "print(len(dataset_formatted['turn_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['once', 'upon', 'a', 'time', ',', 'in', 'a', 'barn', 'near', 'a', 'farm', 'house', ',', 'there', 'lived', 'a', 'little', 'white', 'kitten', 'named', 'cotton', '.'], ['cotton', 'lived', 'high', 'up', 'in', 'a', 'nice', 'warm', 'place', 'above', 'the', 'barn', 'where', 'all', 'of', 'the', 'farmer', \"'s\", 'horses', 'slept', '.'], ['but', 'cotton', 'was', \"n't\", 'alone', 'in', 'her', 'little', 'home', 'above', 'the', 'barn', ',', 'oh', 'no', '.'], ['she', 'shared', 'her', 'hay', 'bed', 'with', 'her', 'mommy', 'and', '5', 'other', 'sisters', '.'], ['all', 'of', 'her', 'sisters', 'were', 'cute', 'and', 'fluffy', ',', 'like', 'cotton', '.'], ['but', 'she', 'was', 'the', 'only', 'white', 'one', 'in', 'the', 'bunch', '.'], ['the', 'rest', 'of', 'her', 'sisters', 'were', 'all', 'orange', 'with', 'beautiful', 'white', 'tiger', 'stripes', 'like', 'cotton', \"'s\", 'mommy', '.'], ['being', 'different', 'made', 'cotton', 'quite', 'sad', '.'], ['she', 'often', 'wished', 'she', 'looked', 'like', 'the', 'rest', 'of', 'her', 'family', '.'], ['so', 'one', 'day', ',', 'when', 'cotton', 'found', 'a', 'can', 'of', 'the', 'old', 'farmer', \"'s\", 'orange', 'paint', ',', 'she', 'used', 'it', 'to', 'paint', 'herself', 'like', 'them', '.'], ['when', 'her', 'mommy', 'and', 'sisters', 'found', 'her', 'they', 'started', 'laughing', '.'], ['what', 'are', 'you', 'doing', ',', 'cotton', '?'], ['i', 'only', 'wanted', 'to', 'be', 'more', 'like', 'you', '.'], ['cotton', \"'s\", 'mommy', 'rubbed', 'her', 'face', 'on', 'cotton', \"'s\", 'and', 'said', 'oh', 'cotton', ',', 'but', 'your', 'fur', 'is', 'so', 'pretty', 'and', 'special', ',', 'like', 'you', '.'], ['we', 'would', 'never', 'want', 'you', 'to', 'be', 'any', 'other', 'way', '.'], ['and', 'with', 'that', ',', 'cotton', \"'s\", 'mommy', 'picked', 'her', 'up', 'and', 'dropped', 'her', 'into', 'a', 'big', 'bucket', 'of', 'water', '.'], ['when', 'cotton', 'came', 'out', 'she', 'was', 'herself', 'again', '.'], ['her', 'sisters', 'licked', 'her', 'face', 'until', 'cotton', \"'s\", 'fur', 'was', 'all', 'all', 'dry', '.'], ['do', \"n't\", 'ever', 'do', 'that', 'again', ',', 'cotton', 'they', 'all', 'cried', '.'], ['next', 'time', 'you', 'might', 'mess', 'up', 'that', 'pretty', 'white', 'fur', 'of', 'yours', 'and', 'we', 'would', \"n't\", 'want', 'that', 'then', 'cotton', 'thought', ',', 'i', 'change', 'my', 'mind', '.'], ['i', 'like', 'being', 'special', '.']]\n",
      "======\n",
      "['what', 'color', 'was', 'cotton', '?']\n",
      "======\n",
      "['once', 'upon', 'a', 'time', ',', 'in', 'a', 'barn', 'near', 'a', 'farm', 'house', ',', 'there', 'lived', 'a', 'little', 'white', 'kitten', 'named', 'cotton', '.']\n",
      "======\n",
      "1\n",
      "======\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(dataset_formatted['passages'][i])\n",
    "print(\"======\")\n",
    "print(dataset_formatted['questions'][i+1])\n",
    "print(\"======\")\n",
    "print(dataset_formatted['answer_spans'][i+1])\n",
    "print(\"======\")\n",
    "print(dataset_formatted['turn_ids'][i])\n",
    "print(\"======\")\n",
    "print(dataset_formatted['answer_sentences'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['passages'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['questions'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['answer_spans'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['answer_sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_formatted['data_points'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "pickler(out_pkl_path,out_pkl_name,dataset_formatted)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
