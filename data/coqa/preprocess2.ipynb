{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "in_pkl_path = \"./\"\n",
    "out_pkl_path = \"./\"\n",
    "word_vocab_pkl_name = \"word_vocabulary.pkl\"\n",
    "char_vocab_pkl_name = \"char_vocabulary.pkl\"\n",
    "\n",
    "if TRAINING:\n",
    "    in_pkl_name = \"dataset_formatted_train.pkl\"\n",
    "#     out_pkl_name = \"dataset_formatted_2_train.pkl\"\n",
    "    out_pkl_name = \"preprocessed_train.pkl\"\n",
    "    GLOVE_FILE_PATH = '/home/bhargav/data/glove/glove.840B.300d.txt'\n",
    "    EMBEDDING_SIZE = 300\n",
    "    GLOVE_STORE = './precomputed_glove.npy'\n",
    "    use_fraction = 0.5\n",
    "else:\n",
    "    in_pkl_name = \"dataset_formatted_dev.pkl\"\n",
    "#     out_pkl_name = \"dataset_formatted_2_dev.pkl\"\n",
    "    use_fraction = 1\n",
    "    out_pkl_name = \"preprocessed_dev.pkl\"\n",
    "    \n",
    "    \n",
    "    \n",
    "pad_symbol = '<pad>'\n",
    "unk_symbol = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, is_padded = False, unk = '<unk>', pad='<pad>'):\n",
    "        self.vocab = Counter([])\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.min_word_count = 2\n",
    "        self.unk = unk\n",
    "        self.pad = pad\n",
    "        self.is_padded = is_padded\n",
    "        \n",
    "    def fit(self,text):\n",
    "        for sent in text:\n",
    "            self.vocab.update(sent)\n",
    "    \n",
    "    def freeze_vocab(self, min_word_count = 5):\n",
    "        self.min_word_count = min_word_count\n",
    "        sorted_counts = sorted(self.vocab.items(), key=lambda x: x[1], reverse = True)\n",
    "        sorted_counts_filtered = [item for item in sorted_counts if item[1] >= self.min_word_count]\n",
    "        for i, item in enumerate(sorted_counts_filtered):\n",
    "            self.id_to_word[i] = item[0]\n",
    "            self.word_to_id[item[0]] = i\n",
    "        \n",
    "        if(not self.is_padded):\n",
    "            self.word_to_id[self.pad] = pad_id = len(self.word_to_id)\n",
    "            self.id_to_word[pad_id] = self.pad\n",
    "#         else:\n",
    "#             assert(self.word_to_id.get(self.pad, -10) == -10)\n",
    "            \n",
    "#         assert(self.word_to_id.get(self.unk, -10) == -10)\n",
    "        self.word_to_id[self.unk] = unk_id = len(self.word_to_id)\n",
    "        self.id_to_word[unk_id] = self.unk\n",
    "            \n",
    "        \n",
    "    \n",
    "    def transform_sent(self, text):\n",
    "        return [self.word_to_id.get(item, self.word_to_id[self.unk]) for item in text]\n",
    "    \n",
    "    def batch_transform(self, text_list):\n",
    "        out = []\n",
    "        for text in text_list:\n",
    "            out.append(self.transform_sent(text))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions might corrupt the input lists. Dont use them after this function is called\n",
    "\n",
    "def pad_context(passages, max_passage_len, max_sent_len, pad_symbol = '<pad>'):\n",
    "    passages_new = []\n",
    "    unpadded_lengths = []\n",
    "    for passage in passages:\n",
    "        new_sents = []\n",
    "        lengths = []\n",
    "        passage = passage[:max_passage_len]\n",
    "        blank_sentence = [pad_symbol]*(max_sent_len)\n",
    "        passage += [blank_sentence]*(max_passage_len - len(passage))    \n",
    "        for sent in passage:\n",
    "            sent = sent[:max_sent_len]\n",
    "            if(len(sent) == 0):\n",
    "                lengths.append(0)\n",
    "            elif(sent[0] == pad_symbol):\n",
    "                lengths.append(0)\n",
    "            else:\n",
    "                lengths.append(len(sent))\n",
    "            sent += [pad_symbol]*(max_sent_len - len(sent))\n",
    "            new_sents.append(sent)\n",
    "        unpadded_lengths.append(lengths)\n",
    "        passages_new.append(new_sents)\n",
    "    return passages_new, unpadded_lengths\n",
    "        \n",
    "\n",
    "        \n",
    "def pad_questions(questions, max_question_len, pad_symbol = '<pad>'):\n",
    "    questions_new = []\n",
    "    for question in questions:\n",
    "        question = question[:max_question_len]\n",
    "        question += [pad_symbol] * (max_question_len - len(question))    \n",
    "        questions_new.append(question)\n",
    "    return questions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = unpickler(in_pkl_path,in_pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "7984\n",
      "7984\n",
      "7983\n",
      "7983\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_original['passages']))\n",
    "print(len(dataset_original['questions']))\n",
    "print(len(dataset_original['answer_spans']))\n",
    "print(len(dataset_original['answer_sentences']))\n",
    "print(len(dataset_original['data_points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_question_lengths = [len(q) for q in dataset_original['questions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpadded_answer_lengths = [len(q) for q in dataset_original['answer_spans']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Paragraphs=====\n",
      "Avg para len:15.548\n",
      "min para len:3\n",
      "max para len:88\n",
      "=====Sentences=====\n",
      "Avg sent len:19.311294057113454\n",
      "min sent len:1\n",
      "max sent len:137\n"
     ]
    }
   ],
   "source": [
    "para_len = []\n",
    "sent_len = []\n",
    "for p in dataset_original['passages']:\n",
    "    para_len.append(len(p))\n",
    "    for s in p:\n",
    "        sent_len.append(len(s))\n",
    "\n",
    "\n",
    "print(\"=====Paragraphs=====\")\n",
    "\n",
    "para_len = np.array(para_len)\n",
    "print(\"Avg para len:{}\".format(para_len.mean()))\n",
    "print(\"min para len:{}\".format(para_len.min()))\n",
    "print(\"max para len:{}\".format(para_len.max()))\n",
    "\n",
    "print(\"=====Sentences=====\")\n",
    "\n",
    "sent_len = np.array(sent_len)\n",
    "print(\"Avg sent len:{}\".format(sent_len.mean()))\n",
    "print(\"min sent len:{}\".format(sent_len.min()))\n",
    "print(\"max sent len:{}\".format(sent_len.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_para_len = 25\n",
    "np.sum(np.greater(para_len,max_para_len))/para_len.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06959094417288397"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len = 40\n",
    "np.sum(np.greater(sent_len,max_sent_len))/sent_len.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg question len:6.478081162324649\n",
      "min question len:0\n",
      "max question len:24\n"
     ]
    }
   ],
   "source": [
    "question_lens = np.array([len(q) for q in dataset_original['questions']])\n",
    "print(\"Avg question len:{}\".format(question_lens.mean()))\n",
    "print(\"min question len:{}\".format(question_lens.min()))\n",
    "print(\"max question len:{}\".format(question_lens.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06763527054108216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len = 10\n",
    "np.sum(np.greater(question_lens,max_question_len))/question_lens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg answer span len:10.05936873747495\n",
      "min answer span len:0\n",
      "max answer span len:337\n"
     ]
    }
   ],
   "source": [
    "answer_lens = np.array([len(q) for q in dataset_original['answer_spans']])\n",
    "print(\"Avg answer span len:{}\".format(answer_lens.mean()))\n",
    "print(\"min answer span len:{}\".format(answer_lens.min()))\n",
    "print(\"max answer span len:{}\".format(answer_lens.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0499749498997996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_answer_len = 25\n",
    "np.sum(np.greater(answer_lens,max_answer_len))/answer_lens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_padded = pad_questions(dataset_original['questions'], max_question_len=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_padded = pad_questions(dataset_original['answer_spans'], max_question_len=max_answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_padded, unpadded_paragraph_len  = pad_context(dataset_original['passages'], max_para_len, max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_sentences_fixed = pad_questions(dataset_original['answer_sentences'], \n",
    "                                       max_question_len=max_para_len,\n",
    "                                      pad_symbol = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_pad_sentence(sent, max_word_len, pad_symbol='<pad>'):\n",
    "    out_sent = []\n",
    "#     sent = sent_in\n",
    "    for word in sent:\n",
    "        if(word == pad_symbol):\n",
    "            out_sent += [pad_symbol] * max_word_len\n",
    "        else:\n",
    "            word = word[:max_word_len]\n",
    "            out_sent += list(word)\n",
    "            out_sent += [pad_symbol] * (max_word_len - len(word))\n",
    "    return out_sent\n",
    "\n",
    "def batch_char_pad_sentence(sent_batch_in, max_word_len, pad_symbol='<pad>'):\n",
    "    padded = []\n",
    "    for sent in sent_batch_in:\n",
    "        sent_padded = char_pad_sentence(sent, max_word_len=max_word_len, pad_symbol=pad_symbol)\n",
    "        padded.append(sent_padded)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(list_of_rows, num_columns):\n",
    "    list_of_columns = [[] for i in range(num_columns)]\n",
    "    for row in list_of_rows:\n",
    "        for i,item in enumerate(row):\n",
    "            list_of_columns[i].append(item)\n",
    "    return list_of_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_questions = batch_char_pad_sentence(questions_padded, max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_answers = batch_char_pad_sentence(answers_padded, max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_char_level_paragraphs(paragraphs_padded,max_word_len):\n",
    "    char_paragraphs = []\n",
    "    for para in tqdm(paragraphs_padded):\n",
    "        padded_para = batch_char_pad_sentence(para, max_word_len=max_word_len)\n",
    "        char_paragraphs.append(padded_para)\n",
    "    return char_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1236.19it/s]\n"
     ]
    }
   ],
   "source": [
    "char_passages_padded = make_char_level_paragraphs(passages_padded,max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_padded_aligned = align(passages_padded, max_para_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_passages_padded_aligned = align(char_passages_padded, max_para_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vocab\n",
      "Size of word vocab:  37212\n",
      "Loading char vocab\n",
      "Size of char vocab:  592\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    word_vocab = Vocabulary(is_padded=True)\n",
    "\n",
    "    #fit vocab on words\n",
    "    for para_group in tqdm(passages_padded_aligned):\n",
    "            word_vocab.fit(para_group)\n",
    "\n",
    "    word_vocab.fit(questions_padded)\n",
    "    word_vocab.fit(answers_padded)\n",
    "\n",
    "    word_vocab.freeze_vocab(min_word_count = 3)\n",
    "\n",
    "    char_vocab = Vocabulary(is_padded=True)\n",
    "\n",
    "    #fit vocab on characters\n",
    "    for para_group in tqdm(char_passages_padded_aligned):\n",
    "            char_vocab.fit(para_group)\n",
    "\n",
    "    char_vocab.fit(char_questions)\n",
    "    char_vocab.fit(char_answers)\n",
    "    \n",
    "    char_vocab.freeze_vocab(min_word_count = 1)\n",
    "    \n",
    "    print(\"Size of word vocab after filtering: \", len(word_vocab.word_to_id))\n",
    "    print(\"Saving word vocabulary\")\n",
    "    pickler(out_pkl_path, word_vocab_pkl_name, word_vocab)\n",
    "    \n",
    "    print(\"Size of char vocab after filtering: \", len(char_vocab.word_to_id))\n",
    "    print(\"Saving char vocabulary\")\n",
    "    pickler(out_pkl_path, char_vocab_pkl_name, char_vocab)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Loading word vocab\")\n",
    "    word_vocab = unpickler(out_pkl_path, word_vocab_pkl_name)\n",
    "    print(\"Size of word vocab: \", len(word_vocab.word_to_id))\n",
    "    print(\"Loading char vocab\")\n",
    "    char_vocab = unpickler(out_pkl_path, char_vocab_pkl_name)\n",
    "    print(\"Size of char vocab: \", len(char_vocab.word_to_id))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: index of unk:37211 , pad:0\n",
      "char: index of unk:591 , pad:0\n"
     ]
    }
   ],
   "source": [
    "print(\"word: index of unk:{} , pad:{}\".format(word_vocab.word_to_id[\"<unk>\"], \n",
    "                                              word_vocab.word_to_id[\"<pad>\"]))\n",
    "\n",
    "print(\"char: index of unk:{} , pad:{}\".format(char_vocab.word_to_id[\"<unk>\"], \n",
    "                                              char_vocab.word_to_id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform words and chars to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_word_idx = word_vocab.batch_transform(questions_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_word_idx = word_vocab.batch_transform(answers_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_char_idx = char_vocab.batch_transform(char_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_char_idx = char_vocab.batch_transform(char_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_paras_to_idx(all_passages, vocab):\n",
    "    paragraphs_idx = []\n",
    "    for sent_group in tqdm(all_passages):\n",
    "        sent_group_idx = vocab.batch_transform(sent_group)\n",
    "        paragraphs_idx.append(sent_group_idx)\n",
    "    return paragraphs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 97.42it/s]\n"
     ]
    }
   ],
   "source": [
    "passages_word_idx = convert_paras_to_idx(passages_padded_aligned, word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.40it/s]\n"
     ]
    }
   ],
   "source": [
    "passages_char_idx = convert_paras_to_idx(char_passages_padded_aligned, char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"passages_word\":passages_word_idx , \"passages_char\":passages_char_idx,\n",
    "            \"questions_word\":question_word_idx, \"questions_char\":question_char_idx,\n",
    "             \"answers_word\":answer_word_idx, \"answers_char\":answer_char_idx,\n",
    "             \"supporting_facts\":answer_sentences_fixed, \n",
    "             \"unpadded_question_lengths\":unpadded_question_lengths, \n",
    "             \"unpadded_passage_lengths\":unpadded_paragraph_len,\n",
    "            \"unpadded_answer_lengths\":unpadded_answer_lengths, \n",
    "             \"data_points\":dataset_original['data_points']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(out_pkl_path, out_pkl_name, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    VOCAB_SIZE = len(word_vocab.word_to_id)\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_FILE_PATH,encoding='utf8')\n",
    "    for line in f:\n",
    "          values = line.split(' ')\n",
    "          word = values[0]\n",
    "          coefs = np.asarray(values[1:], dtype='float32')\n",
    "          embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Read GloVe file\")\n",
    "    \n",
    "    # make sure GloVE doesn't have <unk> and <pad>.  NOTE: These will be handled separately later\n",
    "    # assert(embeddings_index.get('<pad>',-10) == -10)\n",
    "    # assert(embeddings_index.get('<unk>',-10) == -10)\n",
    "\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    print(\"Preparing embedding matrix\")\n",
    "    count_not_found = 0\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "    for word, i in word_vocab.word_to_id.items():\n",
    "        if((word == '<unk>') or (word == '<pad>')):\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            count_not_found += 1\n",
    "            \n",
    "    # initialize <unk> to mean of all embeddings\n",
    "    embedding_matrix[word_vocab.word_to_id['<unk>']] = embedding_matrix.mean(axis = 0)\n",
    "    \n",
    "    print(\"Embedding matrix shape: \",embedding_matrix.shape)  \n",
    "    print(\"Number of words not found in GloVe: \",count_not_found)\n",
    "    print(\"Number of words in GloVe: \", len(embeddings_index))\n",
    "    np.save(GLOVE_STORE, embedding_matrix)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
