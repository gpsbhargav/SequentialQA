{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "out_pkl_path = \"./\"\n",
    "\n",
    "context_history_size = 2\n",
    "\n",
    "if(TRAINING):\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-train-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_train.pkl\"\n",
    "    \n",
    "else:\n",
    "    file_path = \"/home/bhargav/data/coqa/coqa-dev-v1.0.json\"\n",
    "    out_pkl_name = \"dataset_formatted_dev.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return [x.text for x in nlp.tokenizer(normalize(text)) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(text):\n",
    "    paragraph_out = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        paragraph_out.append(word_tokenize(s))\n",
    "    return paragraph_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_overlap(sent,ans):\n",
    "    sent_tok = set(sent)\n",
    "    ans_tok = set(ans)\n",
    "    return 100 - len(ans_tok.difference(sent_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_sentence(passage, span):\n",
    "    matching_sentence = [0 for i in range(len(passage))]\n",
    "    matching_scores = []\n",
    "    for sent in passage:\n",
    "        matching_scores.append(score_overlap(sent, span))\n",
    "    best_match_index = np.array(matching_scores).argmax()\n",
    "    matching_sentence[best_match_index] = 1\n",
    "    return matching_sentence\n",
    "\n",
    "def get_prev_ids(current_index, context_history_size, turn_id):\n",
    "    ids = list(range(current_index-context_history_size, current_index))\n",
    "    assert(turn_id != 0)\n",
    "    for i in range(context_history_size-turn_id+1):\n",
    "        ids[i]=0\n",
    "    return ids\n",
    "\n",
    "def get_rationales(dataset_in):\n",
    "    rationales = []\n",
    "    for passage in tqdm(dataset_in['data']):\n",
    "        for i in range(len(passage['questions'])):\n",
    "            r = normalize(passage['answers'][i]['span_text'])\n",
    "            rationales.append(r)\n",
    "#             rationale_sents = nltk.sent_tokenize(rationale)\n",
    "#             num_sentences_in_rationale.append([len(rationale_sents)])\n",
    "    return rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as file:\n",
    "    dataset_original = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3136.06it/s]\n"
     ]
    }
   ],
   "source": [
    "rationales = get_rationales(dataset_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_in_rationale = [len(nltk.sent_tokenize(r)) for r in rationales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0771639734435676\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(min(num_sentences_in_rationale))\n",
    "print(np.mean(num_sentences_in_rationale))\n",
    "print(max(num_sentences_in_rationale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(num_sentences_in_rationale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hapter xxii northward, along the leeward coast of malaita, the _ariel_ worked her leisurely way, threading the colour riotous lagoon that lay between the shore reefs and outer reefs, daring passages so narrow and coral patched that captain winters averred each day added a thousand grey hairs to his head, and dropping anchor off every walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life.',\n",
       " 'for harley and villa kennan were in no hurry.',\n",
       " 'so long as the way was interesting, they dared not how long it proved from anywhere to anywhere.',\n",
       " 'during this time jerry learned a new name for himself or, rather, an entire series of names for himself.',\n",
       " \"this was because of an aversion on harley kennan's part against renaming a named thing.\",\n",
       " 'a name he must have had, he argued to villa.',\n",
       " 'haggin must have named him before he sailed on the _arangi_.',\n",
       " 'therefore, nameless he must be until we get back to tulagi and find out his real name.',\n",
       " \"what's in a name?\",\n",
       " 'villa had begun to tease.',\n",
       " 'everything, her husband retorted.',\n",
       " \"think of yourself, shipwrecked, called by your rescuers 'mrs.\",\n",
       " \"riggs,' or 'mademoiselle de maupin,' or just plain 'topsy.'\",\n",
       " \"and think of me being called 'benedict arnold,' or ' judas,' or .\",\n",
       " '.',\n",
       " '.',\n",
       " 'or .',\n",
       " '.',\n",
       " '.',\n",
       " \"'haman.'\",\n",
       " 'no, keep him nameless, until we find out his original name.',\n",
       " 'must call him something, she objected.',\n",
       " \"can't think of him without thinking something.\",\n",
       " 'then call him many names, but never the same name twice.',\n",
       " \"call him 'dog' to day, and 'mister dog' to morrow, and the next day something else.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(rationales[222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_counts = [1 if n>1 else 0 for n in num_sentences_in_rationale ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fraction of rationales with more than 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053363397219090565"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sentence_counts)/len(sentence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
