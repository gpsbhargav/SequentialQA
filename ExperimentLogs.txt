Sentence selector on SQuAD:66%. This is like an upper bound on the performance of these models given questions with "complete" information.


=======================================================================================
Word only, 0 history, no regularization, max grad norm=1. lr=0.001. attention=dot
Dev EM=48.92. Train acc ~65 to 75
------------------------
Word only, 0 history, no regularization, max grad norm=1. lr=0.001. attention=concat
Dev EM=43. Train acc ~65 to 75
------------------------
Word only, 0 history, no regularization. lr=0.001. attention=concat (num_units = 150)
Dev EM=45. Train acc ~65 to 75
------------------------
word only, 0 history, no regularization, lr=0.001, attention=concat for ans encoder and sentence selection (num_units = 150)
Dev em = 41. Train acc ~55 to 67
------------------------
word only, 0 history, no regularization, lr=0.001, attention=concat for ans encoder and sentence selection (num_units = 150)
Dev em = 38. Train acc ~60
------------------------
word only, 1 history, no regularization, lr=0.001, attention=dot. 
Dev em = 46. Train acc ~60 to 70
------------------------
word only, 1 history, no regularization, lr=0.005, attention=dot. Weight decay=0.001
Dev em = 26.5. Train acc ~15
------------------------
word only, 1 history, no regularization, lr=0.001, attention=dot. num_birnn_layers=2
Dev em = 40. Train acc 55 to 65
------------------------
word only, 2 history, no regularization, lr=0.001, attention=dot. num_birnn_layers=2
Dev em = 42.9. Train acc ~70
------------------------
word only, 2 history, recurrent_dropout=0.4, lr=0.001, attention=dot. num_birnn_layers=2
Dev em = 39.4. Train acc ~50.
------------------------
word only, 2 history, recurrent_dropout=0.2, lr=0.001, attention=dot. num_birnn_layers=2
Dev em =  43.32. Train acc 60 to 65
------------------------
word+char embeddings, 2 history, recurrent_dropout=0.2, lr=0.001, attention=dot. num_birnn_layers=2
Dev em = 37. Train acc 50 to 60
------------------------
trainable word only, 0 history, recurrent_dropout=0.2, lr=0.001, attention=dot. num_birnn_layers=2
Dev em =  34.4. Train acc 55 to 70
------------------------
word only, 2 history, lr=0.001, max grad norm=1, attention=dot. num_birnn_layers=1
Dev em =  49.8. Train acc 70 to 78
------------------------
word only, 2 history, recurrent_dropout=0.2, lr=0.001, max grad norm=1, attention=dot. num_birnn_layers=2
Dev em =  44.9. Train acc 60 to 70
------------------------


================ Using LSTM instead of GRU ================


word only, 2 history, lr=0.001, max grad norm=1, attention=dot, num_birnn_layers=1, masked attention for sentence representation only
Dev em =  49.1. Train acc 75 to 80
------------------------
word only, 2 history, lr=0.001, max grad norm=1, 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
masked attention for sentence representation only
Dev em =  50.43. Train acc 75 to 83.  Needs regularization
------------------------
word only, 1 history, lr=0.001, max grad norm=1, 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
masked attention for sentence representation only
Dev em =  46.33. Train acc 70 to 82.  Needs regularization
------------------------
word only, 0 history, lr=0.001, max grad norm=1, 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
masked attention for sentence representation only
Dev em =  49. Train acc 75 to 85. Needs regularization
------------------------


================ Masked attention everywhere except history ================


word only, 2 history, lr=0.001, max grad norm=1, Dropout:0.3 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  50.29. Train acc 66 to 77
------------------------
2channel word, 2 history, lr=0.001, max grad norm=1, no dropout
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  42. Train acc 75 to 85. Needs regularization. reducing max grad norm is NOT the issue



================ Tanh instead of ReLU when projecting question to lower dimension ================

word only, 2 history, lr=0.001, max grad norm=1, Dropout:0.3 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  52. Train acc 68 to 76

------------------------
word only, 1 history, lr=0.001, max grad norm=1, Dropout:0.25 
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  49.



================ 
Using answer sentences instead of given answer spans in history,
Removed padding between question and answer in history 
================

word only, 2history, lr=0.001, max grad norm=1, dropout=0.25
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  55.11. 
Without teacher forcing= 47.38
------------------------
word only, 0 history, lr=0.001, max grad norm=1, dropout=0.25
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  51.
Without teacher forcing(the evaluation script)= 50.74

================ 
Gating mechanism while merging question and history
================
word only, 2 history, lr=0.001, max grad norm=1, dropout=0.25
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  56.93.
Without teacher forcing(the evaluation script)= 50.52
------------------------
word only, 2 history, lr=0.001, max grad norm=1, dropout=0.2
attention=multiplicative for history encoding and final question representation 
num_birnn_layers=1, 
Dev em =  56.52.
Without teacher forcing(the evaluation script)= 51.3

================ 
No biLSTM across sentence representations
================
word only, 2 history, lr=0.001, max grad norm=1, dropout=0.2
attention=concat for history encoding and final question representation
Dev em =  55.86
Without teacher forcing(the evaluation script)= 50.69



================ 
No biLSTM across sentence representations
================
word only, 2 history, lr=0.001, max grad norm=1, dropout=0.2
Dev em = 62.4
Without teacher forcing(the evaluation script)= 58.91
Might be overfitting slightly try dropout of 0.3 or 0.4






------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------
------------------------


==============================================================================================================
==============================================================================================================
Using FFNN to get final question representation
==============================================================================================================
==============================================================================================================
word only, 2 history, lr=0.01, max_grad_norm=1, dropout=0.25
Dev em with TF = 56.64
without TF = 41.53%


